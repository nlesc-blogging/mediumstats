<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:cc="http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html">
    <channel>
        <title><![CDATA[Netherlands eScience Center - Medium]]></title>
        <description><![CDATA[We’re an independent foundation with 80+ passionate people working together in the Netherlands’ national centre for academic research software. - Medium]]></description>
        <link>https://blog.esciencecenter.nl?source=rss----ab3660314556---4</link>
        <image>
            <url>https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png</url>
            <title>Netherlands eScience Center - Medium</title>
            <link>https://blog.esciencecenter.nl?source=rss----ab3660314556---4</link>
        </image>
        <generator>Medium</generator>
        <lastBuildDate>Wed, 20 Jul 2022 08:57:21 GMT</lastBuildDate>
        <atom:link href="https://blog.esciencecenter.nl/feed" rel="self" type="application/rss+xml"/>
        <webMaster><![CDATA[yourfriends@medium.com]]></webMaster>
        <atom:link href="http://medium.superfeedr.com" rel="hub"/>
        <item>
            <title><![CDATA[What do entropy, entanglement, cosmology, life, and fake news have in common?]]></title>
            <link>https://blog.esciencecenter.nl/what-do-entropy-entanglement-cosmology-life-and-fake-news-have-in-common-160fb84e0237?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/160fb84e0237</guid>
            <category><![CDATA[life-sciences]]></category>
            <category><![CDATA[conference]]></category>
            <category><![CDATA[cosmology]]></category>
            <category><![CDATA[computer-science]]></category>
            <category><![CDATA[information-theory]]></category>
            <dc:creator><![CDATA[Patrick Bos]]></dc:creator>
            <pubDate>Tue, 19 Jul 2022 08:06:05 GMT</pubDate>
            <atom:updated>2022-07-19T08:06:04.895Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<h4>Report on the Information Universe 4 conference</h4><p><em>(Answer to the above question below obligatory excruciatingly long-winded intro!)</em></p><p>In the past week, I have rediscovered writing. Writing really hurts, you know… I’m pretty sure there should be workplace regulations for pen and paper safety, with regular mandatory hand massages against cramps and such.</p><p>I guess my hand injuries came from overuse, because I filled up two entire notepads with my notes on all the talks at the <a href="https://informationuniverse.astro.rug.nl/2022">Information Universe 4</a> conference.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*f0wzoo1aElRAGFM7raSpdQ.jpeg" /><figcaption>My trusty notepads! As a professional pen-athlete, I should probably have gone with pro materials, though. Moleskine or other brands. Please contact my manager if you want to sponsor my next note-taking event!</figcaption></figure><p>This perhaps seemingly (over)eagerness had two reasons:</p><ol><li>I had to write in the dark, so I was struggling to keep words and lines separated.</li><li>I really wanted to understand everything, since I had promised my colleague, Hanno, I would report on this conference, so there was a lot to write about.</li></ol><p>So, here it is, Hanno! I hope it’ll convince you (and others) to join the next edition, because it was really a joy.</p><h3>Information!</h3><p>So, as I asked in the title, <em>what do entropy, entanglement, cosmology, life, and fake news have in common?</em> The answer is: <strong>information</strong>.</p><p><em>“What do you mean, information?”</em>, you may demand sternly. This was also the topic of many Q&amp;A sessions and coffee breaks. Two main definitions seem to be those of:</p><ol><li>context-free information in the information theoretical and/or statistical sense; and</li><li>something like “meaning” and communication of meaning.</li></ol><p>To communicate meaning, you need some encoding scheme of information (like an alphabet), and a commonly understood standard (like a language). This is probably what most people think of when they talk about information: stuff you can read about, knowledge.</p><p>However, the underlying truth that you’re trying to communicate about is there independent of your messages. Such truths for instance describe the exact state of a bunch of molecules at a certain time: where they are, how fast they move, in what directions, what type of molecules they are, etc., etc.</p><p>Buckets (or canisters, or tubs, or vast intergalactic expanses) of molecules are typically described by statistical physics and thermodynamics, but the kind of statistical description about states of things can be generalized to anything. The unit of information of this kind is the bit.</p><h3>The fundamental building block of the Universe?</h3><p>Now, when you write down the equations of how all this information builds up in a system you get something called <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)"><em>Shannon entropy</em></a>. To be honest, I’m not sure whether I like the word “information” in this context, because it’s actually more about probability. <a href="https://en.wikipedia.org/wiki/Information_content"><em>Surprisal</em></a> seems like a much better word to me. But I digress.</p><p>It turns out that there are several ways information entropy can describe our Universe in ways that solve some of the biggest problems in cosmological physics: these theories naturally unify gravity and quantum physics and explain the mysteries of Dark Matter and Dark Energy.</p><p>At the conference two such theories were presented and discussed:</p><h4>Entropic gravity</h4><p>The theory of <a href="https://en.wikipedia.org/wiki/Entropic_gravity">entropic or emergent gravity</a> by Erik Verlinde and collaborators, in a little over a decade, seems to have moved from wild theoretical speculation into the realm of serious possibility. In several talks, <strong>observational tests</strong> were discussed that interestingly seem to match old alternative theories of gravity like MOND as well.</p><p>Also super interesting was new work by <strong>Manus Visser</strong> on how even Newton’s first and second laws (that describe mass inertia and how anything moves at all) can be explained from entropic principles!</p><p>The math was beyond me, honestly, which is a problem with all of entropic gravity, being based on the information entropy of the entanglement of primordial particles on (or beyond?) opposite parts of the horizon of some kind of anti-de Sitter (AdS) space that supposedly corresponds to our own (conformal space? CF) Universe by means of <a href="https://en.wikipedia.org/wiki/AdS/CFT_correspondence">AdS/CFT correspondence</a>.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/607/0*m3Y7nx8w5CXl6A4k.jpg" /><figcaption>No, really, I’m asking: am I right?</figcaption></figure><p>Still, the predictions are impressive, elegant, enticing and certainly warrant further attention.</p><h4>Wolfram’s Physics project</h4><p>Stephen Wolfram (yes, the Mathematica guy) presented the <a href="https://www.wolframphysics.org/">Wolfram theory of physics</a>.</p><p>The Wolfram Universe starts from a description of space itself as a graph of “space-element” nodes connected to other nodes. Updates to the cells and connections in this graph — representing the passing of time — can be made according to certain rules. Such cell update schemes were inspired by <a href="https://en.wikipedia.org/wiki/Cellular_automaton">cellular automata</a> (which you can very nicely simulate in Mathematica, try it today!).</p><p>Wolfram claimed (I wouldn’t say explained, although I’m sure they try on their website) that when you make this graph big enough and use the right set of update rules, you again get an emergent theory of physics.</p><p>The most interesting part of his talk was that he had some very specific <strong>predictions for things that could be observed</strong> if indeed we live in a Wolfram Universe (did you try Mathematica yet btw? great product…). One was that the 3-dimensional nature of the spacetime we experience isn’t necessarily fixed in the Wolfriverse and we could observe flashes of other-dimensionality somehow. Another thing would be a specific radiation pattern from near the event horizon of a black hole.</p><h3>New cosmological probes and the “Hubble tension”</h3><p>As you may have noticed from the above, my interest piques when information universe theorists start thinking about observability. Theory should be about our actual Universe and it should be falsifiable by observational or experimental probes and tests. Luckily, the conference provided in this area as well.</p><h4>New observations</h4><p>Henk Hoekstra gave a great overview of the Euclid project, which combines a (hopefully soon to be launched) space telescope with ground-based telescope observations to create a new, high resolution map of all the stuff in our local corner of the Universe. “<em>All the stuff</em>” includes dark matter. Of course, we cannot <strong>see</strong> that <em>stuff</em> (hence the name), but using intricate statistical modeling of tiny distortions of light by the gravity of the <em>stuff </em>— a phenomenon known as (weak) gravitational lensing — we can deduce where it’s located.</p><p>Knowing that is really important for understanding all the processes going on and the objects and structures that we see in our neighborhood. Without this knowledge, we simply cannot test our theories.</p><h4>A new probe: strong gravitational lensing of transients</h4><p>A related promising new idea is to use strong gravitational lensing of quasars and even supernovae.</p><p>To understand strong lensing, take a look at a light source through the bottom of a wine glass:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/943/0*Lz8LuSIrVwhxRN6i.png" /><figcaption>Left: DIY strong gravitational lensing. Right: natural lensing by a big galaxy cluster (center) of a background object (blue arcs). From <a href="https://laser.physics.sunysb.edu/_samantha/journal/index.html">https://laser.physics.sunysb.edu/_samantha/journal/index.html</a>, by Samantha Scibelli.</figcaption></figure><p>The light of an object is bent around a heavy foreground object from multiple sides, giving us two (or more) views on a single background object! The light from those two images travelled along different paths, with different distances.</p><p>Now, quasars and supernovae are both <em>transients</em>: objects that change color or brightness over time. If their light travels different distances, we can measure their time-dependent light pattern twice (or more). The punchline is that the differences in the light patterns from the multiple images can tell us something about the Universe! By travelling different distances, or travelling through different environments, their patterns will often be slightly different.</p><p>Kind of a long story, but the bottom line is that this is really promising as a completely<strong> new probe</strong> of the physics of our Universe, one that is <strong>independent of other observational probes</strong> we have used up to now.</p><p>Why is that important?</p><h4>The Hubble tension</h4><p>There’s a fight!</p><p>One of the most important cosmological parameters that describe our Universe as a whole is the <a href="https://en.wikipedia.org/wiki/Hubble%27s_law">Hubble constant</a>. This constant can be measured in several ways. Simplifying a bit, we can say that there are two classes of observations: those from the <em>early-time universe</em> (most prominently: the <a href="https://en.wikipedia.org/wiki/Cosmic_microwave_background">cosmic microwave background</a> (CMB), popularly described as the echo of the Big Bang) and those from the <em>late-time universe</em> (now and just a few billion years ago, e.g. supernova distances).</p><p>For years, different Hubble constant measurements seemed to be converging to a single value (within the error margins). However, with measurements becoming more and more precise (error margins becoming smaller), it now turns out that CMB measurements do not agree at all with late-time measurements!</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/500/1*OKk2JYML7HLFAjzVejBH0Q.jpeg" /></figure><p>This so-called <strong>“Hubble tension”</strong> has grown over the last decade into a serious crisis. Obviously, scientists love a good crisis, because it means that there’s something new to learn just lurking beyond the horizon! The talks by Leon Koopmans, Tamara Davis and Nikki Arendse provided many perspectives on this issue, but no solution has been found yet.</p><p>Improving our understanding of our surroundings in the ways presented by e.g. Job Feldbrugge (who uses analytical models to simplify and better understand the complex universe around us) and Yan-Chuan Cai (who improves modelling of one of the most essential astronomical tools: the redshift as used to estimate distances to objects) will certainly play a role as well.</p><h3>Life, fake news, philosophy and more</h3><p>So far, I’ve only highlighted more or less a third of this conference. The joy of this conference, though, is that not only are we (astro)physicists infused with information theory, but we get to sample from a wide buffet of fields that use information theory in some way too. To be able to have so many world experts in different fields in one place must lead to some really cool cross-domain thoughts and ideas. And it did. Some random highlights:</p><h4>Philosophy</h4><p>Seth Lloyd talked about how Spinoza’s ethics is very suitable for thinking about how we can define ethically “good” computation. This was a really interesting talk from one of the founders of quantum computing. <strong>“Should we fear </strong><a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence"><strong>artificial general intelligence</strong></a><strong> (AGI)?”</strong> was one of his leading questions. Based on Spinoza, Lloyd argued that because…</p><ol><li>joy comes from increasing one’s understanding of the Universe (or God, which is the same to him) and</li><li>individuals can never understand more on their own than together</li></ol><p>…it will be in any intelligence’s interest to cooperate. This goes for us humans, when we work together, but also goes for AGI and anything in between that allows us to increase our intelligence (tools like books, computers, AI, etc.). So: no, we needn’t fear AGI at all… at least, as long as they are Spinozans.</p><p>One interesting random cross-connection from this talk is that Wolfram’s set of all the possible update rules — a concept they call the <a href="https://writings.stephenwolfram.com/2021/11/the-concept-of-the-ruliad/">Ruliad</a>, and which describes all possible Universes — was described in a very similar way to Spinoza’s God. Maybe the similarity is just due to the nature of things that explain everything always everywhere, though.</p><h4>Life</h4><p>Some speakers discussed what life is, whether we should define it at all, how it could have come about, and also how it connects to things like AGI. The talks ranged from quite generic and understandable to very technical, like the talk by David Eric Smith.</p><p>Although it was challenging for me to follow due to lots of jargon, Smith argued convincingly, based on (micro)biological evidence, that ribosomes, the molecules that fold proteins out of RNA, must have come before the actual RNA/DNA that would become the basic information carrier of all life as we know it.</p><p>Digging into this chain of events on how life could have started is a fascinating feat of micro-archaeology, if one may call it that... And one may, if one is to follow the definition of life that Charley Lineweaver gives. He states that essentially biology is a <strong>historical </strong>science. All other definitions of life are problematic and may exclude things like viruses and even humans (we need to eat; how self-sustaining are we?) or include such things as tornados and stars (“systems far from equilibrium”). The study of life is simply a study of how some subset of things we call “living” happened to come about. He contrasts this to physics which tries to uncover time-invariant truths about reality.</p><h4>Fake news</h4><p>Finally, one talk that really hit home for me was the one by Ruurd Oosterwoud on countering disinformation using evidence-based, data-driven solutions.</p><p>Using such solutions may seem like the obvious thing to do, but unfortunately, politically driven solutions often are far from effective. This is no big surprise, because many simply have not been rigorously tested at all. The problem we face is hugely complex, though, with many different actors through many different media with many different motivations and stakes.</p><p>Solutions like fact-checking create merely a ripple in the huge lake of false information that floods us from all sides. And this doesn’t even include yet aspects like intent: do people truly believe what they say or are they actively trying to disrupt any fact-based discussion?</p><p>Oosterwoud presented a number of ways in which we can actually start measuring what works and what doesn’t. One cool example was a website where school children were given the assignment to come up with a fake news item themselves and get as many visitors to it as possible. This gives both the children themselves and researchers insights into successful tactics, which may eventually lead to counter-strategies that actually work.</p><h3>Conclusion</h3><p>You really missed out Hanno!</p><p>Apart from the content and the great discussions over coffee, the conference was held in the DOT full-dome planetarium. Some of the speakers made use of this to show some beautiful full-dome movies or interactive shows.</p><p>It’s one of the most inspiring conferences for technically inclined, but broadly interested people.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=160fb84e0237" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/what-do-entropy-entanglement-cosmology-life-and-fake-news-have-in-common-160fb84e0237">What do entropy, entanglement, cosmology, life, and fake news have in common?</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A single rule for better talks]]></title>
            <link>https://blog.esciencecenter.nl/a-single-rule-for-better-talks-816bb3d422ae?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/816bb3d422ae</guid>
            <category><![CDATA[communication]]></category>
            <category><![CDATA[science-communication]]></category>
            <category><![CDATA[public-speaking]]></category>
            <category><![CDATA[time-management]]></category>
            <category><![CDATA[communication-skills]]></category>
            <dc:creator><![CDATA[Pablo Rodríguez-Sánchez]]></dc:creator>
            <pubDate>Thu, 30 Jun 2022 06:03:04 GMT</pubDate>
            <atom:updated>2022-06-30T11:16:49.152Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p><em>If I had to take a single lesson from event managing, it would be this one: enforce the timing of your events. It can be awkward at times, but it is worth it.</em></p><figure><img alt="A sand clock" src="https://cdn-images-1.medium.com/max/1024/0*SjK1dGbja6UziGox" /><figcaption>Photo by <a href="https://unsplash.com/@alexandar_todov?utm_source=medium&amp;utm_medium=referral">Alexandar Todov</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><h3>A bit of context</h3><p>Since a decade ago, I am part of a <a href="https://naukas.com/">scientific communication society</a>. We gather every year, at least once, in a big theater in Northern Spain in order to perform short, 10-minute talks about science. Each one of these pitches is targeted at a general audience, ranging from professionals to children.</p><p>These events include around 50 talks, divided into 4 sessions, spread over 2 days. This averages to around 12 talks per session. Roughly 2,000 people attend these talks, so it is also important to take regular breaks. Summarizing: an organizational nightmare. Because of this, one of the most remarkable aspects of these events is that we manage to keep the timing under control. In more than a decade of history, the event never had an accumulated delay of more than 5 minutes. How did we do it?</p><p>The answer is simple: every speaker is given 10 minutes and not a second more. And we mean it: there is no way the speaker is going to take more time. This is true regardless of how prestigious the speaker is, how engaged the audience seems to be or how interesting the talk some moderator thinks is. No need to say that the talks start exactly on time, even if part of the audience is late (actually, the doors are closed and nobody is allowed in between talks). Summarizing: the 10 minutes rule is not advisory, but strictly enforced.</p><p>How do we enforce it? Firstly, the 10 minutes rule is enforced by an inanimate, irrational and ruthless clock. A timer, usually on a tablet that the speaker can continuously see from the stage, that will start loudly ringing an alarm after 10 minutes. An alarm that nobody will switch off until the speaker is gone. What will be switched off, instead, is the speaker’s microphone.</p><p>To make things even more interesting, the inanimate, irrational and ruthless clock is in the hands of a very animate, rational, but equally ruthless bouncer, that will not hesitate to jump to the stage and make a very clear cue that the talk is over. Even gently (but firmly) pushing the speaker out of the stage in case of need.</p><h3>Implementing this at your talks or meetings</h3><p>I know this feels rude. Timing and bluntly interrupting each other’s interventions is certainly unacceptable in a conversation with a group of friends. But this is the cornerstone here: a scientific pitch (and the same applies to a professional one, a meeting, a project presentation, …) is not a conversation with a group of friends. It’s a completely different situation that requires a different format and different rules.</p><p>In contrast with a talk with friends, a professional talk is planned and scheduled in detail. People made the effort to adjust their agendas to be able to be there, either as a speaker or as an attendant. When someone takes too much time, they don’t take it from thin air: they take it from other people. And it escalates with the size of your audience: 5 extra minutes in a small audience of 12 people amount to a whole person-hour. Not sticking to an agreed schedule is also a form of rudeness that, in its most dramatic form, leads to the cancellation of the part of the program planned for later in the session. This is certainly more rude and undesirable than clearly notifying everybody in advance about the time limitation and enforcing it.</p><p>It requires, of course, an extra effort from the organizers. To begin with, the rule has to be communicated in advance and enforced equally and fairly. Additionally, the organizer should do as much as possible to make sure the process goes as smoothly as possible. For instance, collect the slides in advance, test them (also in advance, not five minutes before the talk, but with time enough to react in case something goes wrong), and have them ready on the same laptop. Using a clock alarm (I agree that a bouncer may be too much 😅) is a good way of transferring the responsibility of interrupting the speaker to a machine incapable of feeling awkward.</p><h3>Unexpected side effects</h3><p>There are extra benefits of this, perhaps, radical approach. When we applied this to our event, the first surprise was that all the speakers accepted the rule without complaint. Furthermore, the clock almost never rang, and the bouncer very rarely had to stand up.</p><p>But our biggest surprise was that <strong>the quality of the talks improved massively</strong>. Why? Because the stick-to-the-time requirement <strong>forced speakers to rehearse</strong> their pitch at least once. After this, most speakers had to polish their first draft into a much more depurated and neat final version.</p><p>Rehearsing a couple of times feels strange, but it is incredibly powerful. It works (and feels) better if you do it in front of a tiny audience, but you can also do it alone. You will clearly notice the parts that work well, the parts that don’t, and the parts that can be shortened or, even better, removed (see Post scriptum below).</p><p>That’s my personal advice. Let a clock chair your meetings. And let me know in the comments how it went.</p><h3>Post scriptum: remove or not remove?</h3><p>Removal. Omission. Simplification. Probably these words don’t sound like something positive to you (especially if you work in academia). Certainly, you shouldn’t deliberately omit information on a report or an academic publication… but a presentation at a meeting is a different thing. Actually, I have a rule for myself: whenever my presentation slides start looking, sounding and feeling too much like a rigorous academic paper, I take it as a serious warning signal. Because they shouldn’t! Only a paper has to feel like a paper.</p><p>The purpose of most talks is either to persuade, to inspire, or to provide some introductory information about a topic. None of these goals requires high amounts of rigor. Actually, rigor and details often ruin the whole thing.</p><p>If the message you want to communicate actually requires details, then a talk is not the best format to provide it. A report, a paper, or a master class with the active participation of your listeners certainly would be a much better way of conveying your message. <strong>But if you are giving a talk, give just a talk</strong>. And in this case, less is more.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=816bb3d422ae" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/a-single-rule-for-better-talks-816bb3d422ae">A single rule for better talks</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Studying political symbolism in Turkish TV dramas with machine learning]]></title>
            <link>https://blog.esciencecenter.nl/studying-political-symbolism-in-turkish-tv-dramas-with-machine-learning-3938ef40a376?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/3938ef40a376</guid>
            <category><![CDATA[symbolism]]></category>
            <category><![CDATA[politics]]></category>
            <category><![CDATA[nlescssi]]></category>
            <category><![CDATA[turkish-tv-series]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Netherlands eScience Center]]></dc:creator>
            <pubDate>Fri, 10 Jun 2022 07:12:35 GMT</pubDate>
            <atom:updated>2022-06-13T07:40:31.905Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p>By <a href="http://www.twitter.com/pverhaar">Peter Verhaar</a> and <a href="https://www.universiteitleiden.nl/medewerkers/ben-companjen#tab-1">Ben Companjen</a></p><p><em>This blog is part of our blog series: The Small-Scale Initiative on Machine Learning, how did it go?, where groups who were invited to participate in a project with eScience Center Research Software Engineers write about the projects and their experience. This week: </em><a href="http:/www.twitter.com/pverhaar">Peter Verhaar</a> <em>and</em> <a href="http://www.twitter.com/bencomp">Ben Companjen</a><em>. They used Machine Learning to look at political symbols in Turkish television dramas.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*54IeNccLWtlsH1VA" /><figcaption>Photo by <a href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral">JESHOOTS.COM</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>During the last few decades, Turkey has developed into one of the largest exporters of TV Series worldwide. The market share of the Turkish television drama industry currently rivals that of the world’s largest TV production companies in countries such as the United States, India and Mexico. Turkish TV dramas often attract millions of viewers, and they appear to resonate most strongly with audiences in the Middle East, Latin America and Northern Africa. While these TV shows enable the channels that produce and distribute them to rake in sizeable profits, the current Turkish government has also recognised that, because of their popularity, such TV dramas can serve as a powerful tool for wielding political soft power, both at home and abroad.</p><p>Such attempts to exert social and political influence can be observed quite clearly in <em>Payitaht: Abdülhamid (Sultan Abdülhamid)</em>, one of the most prominent TV dramas of the last few years. The series, which has been produced and broadcasted by the state-supported channel TRT since 2017, depicts the life and political career of the last significant Sultan of the Ottoman Empire, Abdulhamid II. The series is set during the late nineteenth and the early twentieth century, but many critics of the show also see strong parallels with contemporary Turkey. It has been suggested that <em>Payitaht: Abdülhamid</em> simultaneously functions as a platform which enables the Erdogan administration to propagate its views on Turkish Islamism as a dominant force in the Middle East.</p><p>These political undercurrents of the TV series <em>Payitaht: Abdülhamid</em> also form the central focus in the PhD research of Mustafa Çolak, an early stage researcher (ESR) in the International Training Network <a href="https://www.itn-mida.org/">Mediating Islam in the Digital Age</a> (MIDA). Çolak’s project is entitled <em>State-Sponsored Turkish Historical Drama Series in Foreign Context</em>, and it is funded, through MIDA, by the Department for Research and Innovation of the European Commission. To analyse the show’s covert political agenda, Çolak concentrates on the symbolism that is used within the various episodes. He is interested, more specifically, in fragments containing symbols which have strong political connotations, such as the star of David, the moon and crescent, which is also shown on the Turkish flag, the pentagram and the keys of heaven. The hypothesis is that, when such symbols are shown alongside Abdülhamid II’s attempts to protect and to modernise the Ottoman empire, this can help to raise support for Turkey’s current protectionist and expansionist policies.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/434/1*KTROmySSDWjbT6cWxjKkrg.png" /><figcaption>Pentagram depicted in the TV series Payitaht: Abdülhamid (directed by Serdar Akar and Emre Konuk)</figcaption></figure><p>A systematic analysis of the symbolism in <em>Payitaht: Abdülhamid</em> can easily turn into a gargantuan undertaking. At the time of writing, the series is in its fifth season, with a total of 154 episodes that last about two hours each. Although these episodes can all be downloaded freely from YouTube, it can clearly be problematic for researchers to free up the time that is needed to watch all of these videos. Even if that hurdle can be overcome, it can be challenging to remain sufficiently sharp and concentrated during these many hours of historical drama.</p><p>Faced with a challenge of this type, Machine Learning can come to the rescue to address arduous issues related to scale. Prompted by the eScience Center’s Small-Scale Initiative on Machine Learning, we formed a project team, which next to Mustafa Çolak, consisted of Ben Companjen, Laurents Sesink and Peter Verhaar from the <a href="https://www.library.universiteitleiden.nl/about-us/centre-for-digital-scholarship">Leiden University Centre for Digital Scholarship</a>, and of Çolaks’s PhD supervisor, Petra de Bruijn. The aim of the project was to develop a reliable method for the recognition of all occurrences of symbols that have political connotations in the first two seasons of <em>Payitaht: Abdülhamid</em>, using algorithms in the field of computer vision. Data about the instances of these symbols can help to perform a ‘distant viewing’ of the series, and to perform comparative analyses of the importance of the concepts and the developments represented by these symbols.</p><p>After the informative workshops and lectures in the kick-off week for the Small-Scale initiative in May 2021, we began to schedule regular meetings with consultants and Machine Learning experts from the eScience Center, to work on the methodology and to discuss our progress. These consultation sessions took place roughly every two weeks. During a first phase of the project, we collected our training data. We downloaded images depicting the selected symbols from Google Images, and, next to this, we also extracted a large number of relevant frames from the first few episodes of <em>Payitaht: Abdülhamid</em>. The training set eventually consisted of about 1500 images.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*emkwo2uk7D1KnUeWp9keOw.png" /><figcaption>Star of David depicted in the TV series Payitaht: Abdülhamid (directed by Serdar Akar and Emre Konuk)</figcaption></figure><p>As we studied the videos we worked with more closely, we also became aware of a number of challenges. The symbols we chose to focus on were often shown from different angles and from different viewpoints. The star of David and the moon and crescent were, in some cases, visible only as blurred shapes in the background. The symbols also looked differently if they were shown on a curved or on a tilted surface. Importantly, it also became clear that two different symbols could be visible at the same time on a single video frame. On the basis of this latter finding, we estimated that it would not be useful to develop separate binary classification models for each of the symbols we focused on. Instead, we chose to work on a single categorical classification model, which could produce prediction values for all of these symbols simultaneously.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/860/1*y83LGI_IeyCIJJkGKk93SA.png" /><figcaption>Moon symbol depicted in the TV series Payitaht: Abdülhamid (directed by Serdar Akar and Emre Konuk)</figcaption></figure><p>These images have subsequently been used to train a convolutional neural network. Following advice we received during the consultation meetings with the Machine Learning experts from the eScience Center, we decided to make use of Transfer Learning. Transfer Learning is a technique in which an existing pre-trained model is reused and repurposed. This approach was productive in our situation, indeed, because the set of training data we created was still relatively small. In our project, we made use of the weights assigned in the Xception model, which consists of 71 layers and which can classify into 1000 classes. For our project, we only retrained the final layer of this model, using the procedure that is explained in <a href="https://keras.io/guides/transfer_learning/">the page about Transfer Learning on the Keras website</a>. The training process, which comprised a sequence of 200 epochs, was carried out on Leiden University’s <a href="https://www.universiteitleiden.nl/en/research/research-facilities/alice-leiden-computer-cluster">infrastructure for High Performance Computiung named ALICE</a>. We achieved an accuracy of 83% on the validation set, which consisted of 30% of the total dataset.</p><p>Working with this model, we were eventually able to create data about all the politically charged symbols that could be recognised in the first two seasons of <em>Payitaht: Abdülhamid</em>. These data proved to be very useful for Çolak’s research project. The findings helped him to substantiate the claim that there are strong thematic parallels between the Ottoman Empire that is portrayed in the series on the one hand and the Turkey we can witness today on the other. The preliminary results of this project were presented during a workshop entitled <a href="https://www.lorentzcenter.nl/the-turks-are-coming-the-popular-outreach-of-turkish-tv-series.html">The Turks are Coming!</a>, which was organised from 6 to 10 December 2021 at the Lorentz Center in Leiden. The workshop was attended by an international group of researchers interested in the socio-political impact of Turkish television series. On the whole, the responses to this presentation were very favourable.</p><p>In this experiment, we tried to teach a computer to recognise political symbols. This was very interesting, and we were also very pleased to notice, during the final stages of the project, that the approach we had implemented also resulted in useful and valuable research findings. These accomplishments can also be attributed in large part to the shrewd and generous support we received from the consultants of the eScience Center. We had a basic understanding of machine learning and of computer vision before the start of this project, but we also knew that there still was much to learn for us about these complicated topics. The consultation sessions offered by the eScience Center certainly helped to flatten the learning curve. During these lively meetings, we discussed snippets of code we had developed, and we were often given invaluable advice about the parameters for the various functions we worked with. If we had needed to find the optimal settings for all these functions and parameters on our own, this would undoubtedly have taken us many iterations of trials and countless errors. The fact that the consultants could simply tell us the best settings for the activation functions, for instance, has eventually saved us enormous amounts of time. All in all, this collaboration with the eScience Center was extremely productive, as it helped us to make much more progress, and in much less time.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=3938ef40a376" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/studying-political-symbolism-in-turkish-tv-dramas-with-machine-learning-3938ef40a376">Studying political symbolism in Turkish TV dramas with machine learning</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[The mystery of glass: why machine learning can help us]]></title>
            <link>https://blog.esciencecenter.nl/the-mystery-of-glass-why-machine-learning-can-help-us-c1b9690565a3?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/c1b9690565a3</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[glass]]></category>
            <category><![CDATA[nlescssi]]></category>
            <category><![CDATA[material-science]]></category>
            <category><![CDATA[deep-neural-networks]]></category>
            <dc:creator><![CDATA[Netherlands eScience Center]]></dc:creator>
            <pubDate>Mon, 23 May 2022 16:10:34 GMT</pubDate>
            <atom:updated>2022-05-23T16:35:01.504Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p>By Ilian Pihlajamaa</p><p><em>This blog is part of our blog series: The Small-Scale Initiative on Machine Learning, how did it go?, where groups who were invited to participate in a project with eScience Center Research Software Engineers write about the projects and their experience. This week: </em>Ilian Pihlajamaa <em>from the</em> <a href="http://tps.phys.tue.nl/janssen/">Non-Equilibrium Soft Matter group of Liesbeth Janssen</a>: <em>They trained a deep neural network to predict the properties of glass-like materials that have not been produced before. In just a few milliseconds.</em></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*HK_lRdLKr8YtS5CU" /><figcaption>Photo by <a href="https://unsplash.com/@quinoal?utm_source=medium&amp;utm_medium=referral">Quino Al</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>In school, you may have been taught that there are three main states of matter: the gas, liquid and solid state. In the gas and liquid states, molecules or atoms are free to move around, whereas in the solid state, particles are stuck in place and form a three-dimensional geometric structure we refer to as a <em>crystalline lattice</em>.</p><p>While this is an accurate description for most materials, there are some that do not fit it. Glasses are a prime example. A glass is created by cooling down a liquid sufficiently fast that the constituent particles do not have time to order themselves in a lattice, but get stuck in the place that they happened to occupy when the material was still a liquid. The resulting material is a solid (otherwise we couldn’t use it to drink from), but its microscopic structure is disordered. In material science, the word “glass” does not only refer to the glass that we are used to, but also to many other disordered solids. When we think of it this way, any liquid can form a glass. Some examples are plastics, rubbers, ceramics and many metals.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*0Anxd3YW-rHn4Vnyljj6KQ.png" /></figure><p>In contrast to the standard crystallization transition (from liquid to solid), the glass transition is not understood by science. As Nobel laureate Phillip W. Anderson put it in 1995: “The deepest and most interesting unsolved problem in solid state theory is probably the theory of the nature of glass and the glass transition.” Many research groups around the world are working on theories that try to quantitatively predict the material properties of a glass, given some information on the microscopic arrangement of the particles. One of these theories, developed in the 80s, is called Mode Coupling Theory. It does an excellent job of describing the way in which the viscosity (“thickness”) of a liquid grows when it is rapidly cooled down, but fails to accurately predict at what temperature the liquid turns into a glass. This failure is due to a small number of approximations that are made in the theory. Recently, a method was proposed to systematically correct the approximations that Mode Coupling Theory makes. The resulting new theory, called Generalized Mode Coupling Theory, clearly does a better job than standard Mode Coupling Theory, and looks like it agrees very well with experimental data and simulations:</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*u_crRy_Nw1JvYArBS9G70Q.png" /></figure><p>We say “looks like” since we are not actually sure what the results of the full Generalized Mode Coupling Theory are, because its equations are too difficult to solve by hand and must therefore be solved approximately on a computer.*</p><p>Together with the eScience Center, as a part of the Small-Scale Initiative in Machine Learning, we set out to come up with a more intelligent approach to solving these equations. Instead of solving them by brute force on a supercomputer, we trained a deep neural network that predicts the solution of these equations for any order in milliseconds. In order to do so, we first engineered a large data set of ten thousand fictional materials for which we solved the full GMCT equations up to a randomly chosen order between 1 and 5 by brute force. We were then able to train a deep neural network with roughly one hundred thousand free parameters to reproduce these calculations with an average error of less than one per cent.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*XzU0STC7jp-IB4G6GXlImw.png" /></figure><p>Because of the instrumental speed-up that this machine learning method brings, compared to a brute force calculation, it is now possible to explore the glass forming behavior of new materials with an unprecedented speed. As a test, we used the deep neural network to study a sticky hard spheres liquid. This is a model that is commonly used to study many kinds of large particles that attract each other, and displays very interesting behavior and gives fundamental insights into the dynamics of actual particles. Even though the machine learning model had never seen a sticky hard sphere liquid before, it was able to predict the glass transition curve for this material with an error of only four per cent compared to the brute force method, which took almost one hundred thousand times as long to compute.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*icjlF-nR52aXe0FTqYmDjw.png" /></figure><p><em>*For math-savvy readers, it involves an infinite-dimensional coupled hierarchy of nonlinear integro-differential equations. The main challenge here is the fact that it these equations form an infinite-dimensional hierarchy. How we approach this in practice is that we solve the hierarchy only up to a certain level, and throw away everything beyond that. If we throw away everything beyond just the first equation, we recover standard Mode Coupling Theory from the 80s. If we take into account two equations, we call it 2nd order GMCT, and so on. Currently, using modern computing techniques, we have been able to solve GMCT explicitly up to the 5th order, which takes a few days on a high performance computing cluster. Each order beyond that increases the computational time needed to solve the equations roughly by a factor of 50, which gets out of hand very quickly.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=c1b9690565a3" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/the-mystery-of-glass-why-machine-learning-can-help-us-c1b9690565a3">The mystery of glass: why machine learning can help us</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Reusable software in the era of AI: why sometimes you must reinvent the wheel]]></title>
            <link>https://blog.esciencecenter.nl/reusable-software-in-the-era-of-ai-why-sometimes-you-must-reinvent-the-wheel-306036754bec?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/306036754bec</guid>
            <category><![CDATA[ai]]></category>
            <category><![CDATA[reusable]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[software]]></category>
            <dc:creator><![CDATA[Sonja Georgievska]]></dc:creator>
            <pubDate>Fri, 13 May 2022 07:39:40 GMT</pubDate>
            <atom:updated>2022-05-13T10:27:01.032Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<blockquote>In traditional software development, reinventing the wheel is a cardinal sin. But AI has changed the picture. In AI-based software development the traditional reusable components, like data aggregation, are only the beginning. What about the actual AI pipeline, can we make that reusable? Should we <em>want to</em> invest our time in reusable AI?</blockquote><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*fxCP5veXW7Lagbci" /><figcaption>Reusable components. Photo by <a href="https://unsplash.com/@girasolmom?utm_source=medium&amp;utm_medium=referral">Sandra Harris</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>When building software from scratch, in business or academia, a very desirable property of the software is for it to be <a href="https://link.springer.com/conference/icsr">reusable</a>. This means that it is not meant to be used for one particular task, one business client or one scientific problem. Ideally, one would like the software to be built ground-up with re-usability in mind: allow for the tool to be able to operate on different data types, different context and different future scenarios that cannot be considered at the moment. In business, this approach maximizes profit, as many clients can be acquired with the same tool. Each business case would require some customization, but the backbone of the software remains the same and stable. In academia, reusability of tools or components allows for wider scientific impact.</p><p>Contemporary software solutions are increasingly based on “Artificial Intelligence” (AI) models. It is tempting to explore how much of the AI based tools can be reusable and applied in a different context. This text attempts to break down and take a closer look at the AI-based software development process and to discuss potentials and pitfalls when trying to make AI reusable.</p><h4>How modern AI systems work</h4><p>What is AI today? It is mostly a system that relies a lot on deep neural networks. In the past decades, depending on fulfilled or unfulfilled promises of different methods, AI meant different things, but in 2022 certainly AI is a system that uses one or more deep neural networks to come with results.</p><p>What is a deep neural network? It is a complex function F that maps input to output. Input or output can be any object that you can think of.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*oopxG6xQfBVnmDAS" /><figcaption>Mapping input to output. Try all possible connections until it works? Photo by <a href="https://unsplash.com/@thevictorbarrios?utm_source=medium&amp;utm_medium=referral">Victor Barrios</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Every function has shape and concrete parameters. For example,</p><p>F(x) = 3x² + 4x +1</p><p>has the shape of a quadratic function with parameters 3,4,1. Note that there is an infinite number of possible shapes that a function can take. The shape of a deep neural network is, in principle, very complex.</p><p>Different shapes work for different types of input data. That’s because data type can be a video or a molecule or a piece of text, or a collection of the previous.</p><p>In deep learning, the task of a team of engineers is to craft a shape of the function that is suitable to the kind of input data, and to the problem that it is trying to solve. Or to adapt (preprocess) the input data for a known shape, based on knowledge about the data, the problem, and shapes. The parameters are learnt automatically by the computer, therefore “machine learning”. The parameters are learnt from a lot of data, for which X and F(X) are known. In order for the computer to be able to learn the parameters, the engineer has to guide it with an appropriate “loss” or optimization function, that computes the “difference” between the predicted F(X) and the actual F(X). Note that, for a particular X, the loss function is a function of the parameters of F. The loss function incorporates the optimization strategy, based on (again) the problem and data.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*rmutMCsM9DcmafzT" /><figcaption>What’s <strong>your</strong> strategy? Photo by <a href="https://unsplash.com/@jeshoots?utm_source=medium&amp;utm_medium=referral">JESHOOTS.COM</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>In time, with enough data, the computer learns the parameters that minimize the value of the loss function, that is, match best X and F(X) for any X from the domain of F. This “time” can be reasonably finite, unreasonably long but still finite, or infinite. That depends on how well the loss function was designed, how well the shape of F was crafted, and how well the input data was adapted for the problem.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*H7AH31piD9B-ZNcg" /><figcaption>Time is a component, too. Non-reusable. Photo by <a href="https://unsplash.com/@goumbik?utm_source=medium&amp;utm_medium=referral">Lukas Blazek</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>For all of those ingredients to be there, the team needs <a href="https://blog.esciencecenter.nl/small-scale-initiative-in-machine-learning-2021-how-did-it-go-9978a70b5b1">combined knowledge</a> and understanding of</p><ul><li>the problem and data, or<em> domain knowledge</em>,</li><li>various shapes of F, or <em>neural network architectures</em>,</li><li>mathematical optimization — for the loss function and related,</li><li>statistical methodology — to make sure that F will perform well on future data,</li><li>existing software frameworks for deep learning, and, of course,</li><li>programming.</li></ul><p>The more knowledge, the better, but it cannot be measured or quantified. This is what is called “expert” knowledge, built over years of education, training and/or experience.</p><p>Note that for every problem a <em>customized</em> shape, a <em>customized</em> adaptation of the input data, and a <em>customized</em> loss function is needed. This is creative process and takes some time for understanding and investigation. Otherwise, if one uses a generic shape or a generic input data adaptation or a generic loss function, the time it would take for a computer to learn the parameters well could easily become <a href="https://blog.esciencecenter.nl/how-not-to-use-deep-learning-in-science-e984b02a4df0">infinite</a>. (In this case, it may be concluded that “the network cannot learn well”, or “there is not enough data”. It can be difficult to dispute these conclusions, though, especially if there is no reference point.)</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*CgfhCugQCpo4ZuBb" /><figcaption>Making AI can feel like climbing up infinite stairs. Photo by <a href="https://unsplash.com/@flub?utm_source=medium&amp;utm_medium=referral">Maxime Lebrun</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Bellow is a schematic representation of the process of machine learning, the model being usually the shape of F (or the neural net architecture). Note that the process can be iterative: most of the time it is not linear.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*WlD44RaYM6VTcvA-" /><figcaption>This image by <a href="https://medium.com/@s.georgievska">Sonja Georgievska</a> is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC by 4.0</a></figcaption></figure><h4>Enter reusability</h4><p>Suppose you want to make a reusable software tool for AI, a software that can be reused in another context or for another problem. What does it mean? Which parts of the AI system can be completely automated and reused?</p><p>The data aggregation process can be made reusable, for example, for multiple problems that use the same type of data. You can even use generic data objects that can hold any type of data; taking into account that further along the process you will need a customized component that prepares the data for the particular neural network architecture. (Note that, however, data aggregation has nothing to do with AI; data does not need AI, it’s the other way round.)</p><p>The data adaptation is more tricky, because as we pointed out above, it is specific to the actual problem, so choices being made here influence the end result (finite vs infinite training). The data adaptation also depends on the data distribution: datasets may have the same type but different distribution. Not taking into account the data distribution leads to biased, or <a href="https://blog.esciencecenter.nl/ai-will-not-steal-your-job-heres-why-d59231eac0ef">irresponsible AI</a>. This leaves very little space for a generic reusable component here. The more you want to reuse, the more assumptions you are making, that may hurt you in the long run.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*xdWZT-pRN4VbdV7j" /><figcaption>How is <strong><em>your</em></strong> data distrubuted? Photo by <a href="https://unsplash.com/@lukechesser?utm_source=medium&amp;utm_medium=referral">Luke Chesser</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Then, model (neural net architecture) crafting. You can choose to make it from scratch in one of the generic deep learning frameworks. But then, we saw that the model is specific to the problem and dataset. If you want to make a reusable component here, you can make a model-generator, that would give the user a choice, manual or automated, of a plethora of models suitable for the problem. This generator takes some <strong>time</strong> to build; yet, your final result in terms of accuracy will be as good as you can get from the pre-defined choice of models. Your model will not be the state-of-art model for the particular problem and data type, nor will it include expert insights about the problem and data. This automation or reusability saves users time at the expense of the quality of results. Instead of potentially 98%, your model will have an accuracy of 91%.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*51Plh81AdqTfFgbH" /><figcaption>Please, choose a model that satisfies specifications. Photo by <a href="https://unsplash.com/@bright?utm_source=medium&amp;utm_medium=referral">Karen Vardazaryan</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Then you have the loss function. This is a small piece of code that is either highly adapted to the actual problem, or one uses one of the pre-existing (and reusable) loss functions in the deep learning framework, but then with a <a href="https://blog.esciencecenter.nl/how-not-to-use-deep-learning-in-science-e984b02a4df0">“fingers crossed”</a> strategy. If overlooked, it can also produce “biased AI”.</p><p>Model training is already fully automated by the deep learning framework, that is the “machine learning” process.</p><h4>So…</h4><p>We saw that the only part that can produce a reusable component without affecting the final results is the data aggregation process. In the stages that follow, every time you use something off the shelf, you are doing it at the expense of the quality of the final results. On the other hand, most of the workload in the following stages is intellectual rather than programming. The data-adaptation code is a Python script that calls standard libraries; but you have to know exactly what you are doing to your data. The model is also a few hundreds of lines of code. The loss function is usually a few lines of code. It is at most tens of lines of code, if you are encoding your (very customized) domain knowledge into it. The trained model could be gigabytes of automatically generated machine-readable, and not human-readable, high-dimensional matrix. Ironically, before thinking about reusability of the <a href="https://www.infoworld.com/article/3644968/how-no-code-reusable-ai-will-bridge-the-ai-divide.html">trained model</a>, and making your model transferable, let us point that it is already <a href="https://blog.esciencecenter.nl/machine-learning-when-it-is-easy-when-it-is-difficult-9de0e1129593">challenging</a> enough to have the model re-usable on future data for which it was originally meant.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/700/0*JtDR94c8PMnvwFa3" /><figcaption>The bulk of your AI software is not human-readable. Photo by <a href="https://unsplash.com/@comparefibre?utm_source=medium&amp;utm_medium=referral">Compare Fibre</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>Thus, will you think twice next time you want to build a reusable AI software? Is your goal to have a tool that can show proof-of-concept but never be actually used in production mode? Then, invest your time in making it reusable. Is your goal to have an AI tool that will make profit, “beat” your competition or give state-of-art science results? Then, better invest less time on the keyboard and more time on the whiteboard.</p><p><em>A special thanks to </em><a href="https://egpbos.medium.com/"><em>Patrick Bos</em></a><em>, </em><a href="https://www.esciencecenter.nl/team/dr-tom-bakker/"><em>Tom Bakker</em></a><em> and </em><a href="https://www.esciencecenter.nl/team/dr-lieke-de-boer/"><em>Lieke de Boer</em></a><em> for improving the post.</em></p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=306036754bec" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/reusable-software-in-the-era-of-ai-why-sometimes-you-must-reinvent-the-wheel-306036754bec">Reusable software in the era of AI: why sometimes you must reinvent the wheel</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Forget about Jupyter Notebooks, showcase your research using Dashboards]]></title>
            <link>https://blog.esciencecenter.nl/forget-about-jupyter-notebooks-showcase-your-research-using-dashboards-5d13451ba374?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/5d13451ba374</guid>
            <category><![CDATA[dashboard]]></category>
            <category><![CDATA[research-data]]></category>
            <category><![CDATA[jupyter]]></category>
            <category><![CDATA[streamlit]]></category>
            <category><![CDATA[python]]></category>
            <dc:creator><![CDATA[Stef Smeets]]></dc:creator>
            <pubDate>Fri, 29 Apr 2022 08:47:48 GMT</pubDate>
            <atom:updated>2022-05-04T06:39:08.956Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<h3>Forget about Jupyter Notebooks — showcase your research using Dashboards</h3><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*3HbRY0-Jj0gCHU2W" /><figcaption>Photo by <a href="https://unsplash.com/@condorito1953?utm_source=medium&amp;utm_medium=referral">Arie Wubben</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>As a Python lover, I use <a href="https://jupyter.org/">Jupyter Notebooks</a> for everything. Notebooks mix markdown, code, and inline plots, which makes them a fantastic tool for exploratory data analysis. I use them to develop and share code, prototype new ideas, explore libraries, play around with data, and create plots and visualizations. Notebooks can be rendered as static html and pdfs, so they are also excellent to write reports, documentation and tutorials… when the intent is to share the code alongside the data.</p><p>As a researcher, however, I find that the code sometimes gets in the way of the data I want to show. So when the intent is to share the data with a non-technical audience, what options do we have? Are there better alternatives?</p><h3>The problem with notebooks</h3><p>Before we continue, let’s take a step back and look at a few of the problems with Jupyter Notebooks.</p><p>There is no denying that Jupyter notebooks have become very popular over the last years to present research results. This means that the issues with Jupyter Notebooks are <a href="https://towardsdatascience.com/5-reasons-why-jupyter-notebooks-suck-4dc201e27086">well</a> <a href="https://www.youtube.com/watch?v=7jiPeIFXb6U">known</a>. My biggest gripes are:</p><ol><li>The non-linear execution model. Notebooks often contain hidden state that is difficult to reason about. This can make them difficult to use for beginners.</li><li>Notebooks are difficult to share with a non-technical audience. They would need to know how to set up Python, install libraries, manage environments, and modify the code.</li></ol><p>Notebooks can be hosted somewhere like <a href="https://mybinder.org/">binder</a> (solving 2.), making the code immediately reproducible. Reproducible, only <em>if</em> one understands the code and how to run the notebook (including its quirks, see 1.). Here, the inline code can get in the way of itself. I have seen many notebooks spelling out what each variable does and how to run the code at the top. You would need some understanding of how the code works to use the notebooks.</p><h3>Dashboards</h3><p>That is where dashboards come into play.</p><p>Dashboards are a relatively new concept coming from the data science world making use of the modern web. In essence, dashboards are simple web-apps used to quickly glance at some data. Like a mini graphical interface for your data.</p><p>At the Netherlands eScience center, we love using dashboards to <a href="https://blog.esciencecenter.nl/https-blog-esciencecenter-nl-spot-visual-scientific-data-analytics-made-easy-62e03a895bae">present</a> <a href="https://blog.esciencecenter.nl/coalition-polls-for-the-people-with-coalitiewijzer-68bca83b95e7">our</a> <a href="https://blog.esciencecenter.nl/storyboards-for-science-communication-85e399e5c1b5">data</a>.</p><p>If you are using Python, these are the ones you should be looking at:</p><ul><li><a href="https://plotly.com/dash/">Dash</a> (2017, 883k downloads/month)</li><li><a href="https://panel.holoviz.org/">Panel</a> (2018, 387k downloads/month)</li><li><a href="https://streamlit.io/">Streamlit</a> (2019, 930k downloads/month)</li><li><a href="https://voila.readthedocs.io/en/stable/">voila</a> (2019, 56k downloads/month)</li></ul><p>All are excellent choices. For a full comparison, check out <a href="https://medium.datadriveninvestor.com/streamlit-vs-dash-vs-voil%C3%A0-vs-panel-battle-of-the-python-dashboarding-giants-177c40b9ea57">this blog post</a>.</p><h3>Streamlit</h3><p>Out of these four, <a href="https://streamlit.io/">Streamlit</a> stood out to me the most for its ease of use. For one of my projects, I have been playing with it to develop a simple data processing GUI. These are my initial impressions.</p><ol><li>After having used it for a week, I found it extremely straightforward to get started with.</li><li>The linear execution model makes it easy to reason about the code (more on this <a href="#How-does-this-work">later</a>).</li><li>There is no need to know any web development, as one of the goals of the library is to look good out-of-the box (spoiler alert: it does).</li><li>The api is well-designed, easy-to-manage, and very Pythonic. You could get to grips with the entire API in a day. Some would say the API is limited, but in my opinion it has a very clear scope which fits my brain. It also helps that the <a href="https://docs.streamlit.io/">documentation</a> is well-structured, with clear explanations and <a href="https://streamlit.io/gallery">examples</a>.</li><li>The streamlit developers claim its the fastest way to build data apps in Python. This sounds like a salespitch, but it might be true. You can turn any Python script into an interactive dashboard in minutes.</li></ol><h3>From a normal plot…</h3><p>Let’s have a look at an example. As a researcher, I had plenty of Python scripts or notebooks lying around which just did this:</p><ol><li>Load or generate some data</li><li>Apply some operations to the data</li><li>Make some plots</li></ol><p>I would endlessly tune the the parameters and re-run the script to get the right plot. No problem for me. But, when it came to sharing the scripts with my not-so-software-savvy colleagues, this meant taking on a support role. Think about setting up Python, managing environments, fixing bugs, feature requests, etc…</p><p>Sounds familiar?</p><p>The snippet below generates some data (a normal distribution), fits it, and creates a <a href="https://matplotlib.org/">matplotlib</a> plot out of it. It takes three parameters, mu_in, std_in, and size.</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/201b0fba772f1585c32ca202ac274a95/href">https://medium.com/media/201b0fba772f1585c32ca202ac274a95/href</a></iframe><figure><img alt="" src="https://cdn-images-1.medium.com/max/640/1*EIzolDuIcUxBHOjEzv58Mg.png" /><figcaption>A normal plot</figcaption></figure><h3>…to a fancy dashboard</h3><p>Let’s turn this into an interactive dashboard in four simple steps:</p><ol><li>import streamlist as st 😅</li><li>Add a title using <a href="https://docs.streamlit.io/library/api-reference/text">st.title</a></li><li>Turn the input parameters into interactive sliders using <a href="https://docs.streamlit.io/library/api-reference/widgets">st.slider</a></li><li>Tell streamlit about our plot using <a href="https://docs.streamlit.io/library/api-reference/charts">st.pyplot</a></li></ol><p>Note that we do not have to change any of the data generation, fitting, or plotting code!</p><iframe src="" width="0" height="0" frameborder="0" scrolling="no"><a href="https://medium.com/media/65c4edb568ab7f12b9858e058b1f9469/href">https://medium.com/media/65c4edb568ab7f12b9858e058b1f9469/href</a></iframe><p>Then run the dashboard using:</p><pre>streamlit run my_dashboard.py</pre><p>This will start a server, and the dashboard can be accessed through the browser (much like a Jupyter Notebook).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/780/1*K-Fgf6aFQ_CoAqsqH5lIbQ.png" /><figcaption>A fancy dashboard</figcaption></figure><h3>How does this work?</h3><p>The way Streamlit works is quite interesting. Everytime a slider is moved, a box is checked, or a button is pressed, Streamlit triggers a re-run of the script. The input values are updated. The javascript back-end keeps track of the values.</p><p>This means that the code itself executes linearly. In my view, this simplicity is what sets it apart. There is no need for any callbacks or complex flow controls. Your python scripts runs from top-to-bottom. This makes it easy to reason about the code. And with minimal modifications to the python code, any script can be turned into a dashboard.</p><p>Are there any downsides? Yes. Because streamlit re-runs the entire script on every update, it can feel a bit slow. Especially when updating a large number of plots. It can also get stuck on long-running functions. For performance optimizations, streamlit has some options to <a href="https://docs.streamlit.io/library/api-reference/performance">cache the result</a>.</p><h3>Plotting libraries</h3><p>The example above uses <a href="https://matplotlib.org/">matplotlib</a> for the plots. Matplotlib has been the go-to plotting library for Python for many for a long time. It has been around for nearly two decades, and it is tighly integrated in the scientific python stack.</p><p>If you are familiar with matplotlib, you will know that it is great for making making publication quality plots. You will also know that making interactive plots can be a hassle.</p><p>Streamlit supports these libraries:</p><ul><li><a href="https://matplotlib.org/">matplotlib</a></li><li><a href="https://altair-viz.github.io/">altair</a></li><li><a href="https://bokeh.org/">bokeh</a></li><li><a href="https://plotly.com/python/">plotly</a></li><li><a href="https://seaborn.pydata.org/">seaborn</a></li><li><a href="https://deckgl.readthedocs.io/en/latest/layer.html">PyDeck</a></li><li><a href="https://github.com/xflr6/graphviz">GraphViz</a></li></ul><p>Modern plotting libraries like <a href="https://plotly.com/python/">plotly</a>, <a href="https://bokeh.org/">bokeh</a>, and <a href="https://altair-viz.github.io/">altair</a> render directly to javascript. This means they are built for the web, and interactivity is built-in. This makes them better suited for web-apps. If you are going to make a dashboard, I recommend checking out one of these alternatives.</p><h3>Sharing your dashboard</h3><p>Alright, so now that we have made a fancy looking dashboard, so that anyone can play with the data. How do we make it available?</p><p>Streamlit uses a host/server model, which means you can run it on your own server.</p><p>Easier is to use the <a href="https://streamlit.io/cloud">streamlit cloud</a> to host your dashboard (it’s free for students and open-source projects). I found this also quite straightforward to set up. All I had to do was to create a <a href="https://github.com/stefsmeets/dashboard_blog">repository on github</a> with the code and a requirements file.</p><p>Then I logged into <a href="https://share.streamlit.io/">streamlit cloud</a> using the Github SSO, and started a new app pointing at my repo and code.</p><p><a href="https://share.streamlit.io/stefsmeets/dashboard_blog/main">Click here</a> for the result! 🥳</p><h3>Final remarks</h3><p>In this blog post, I introduced streamlit and showed how it can be used to turn a python script into a dashboard, and host it online. An excellent way to showcase your research to a non-technical audience, if you ask me. The linear execution model makes it straightforward to adapt existing scripts. The code does not get in the way, and the result looks awesome.</p><p>So next time you want to present some data in a notebook, consider using a dashboard instead.</p><p>All the code in this blog post is available from <a href="https://github.com/stefsmeets/dashboard_blog">Github</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=5d13451ba374" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/forget-about-jupyter-notebooks-showcase-your-research-using-dashboards-5d13451ba374">Forget about Jupyter Notebooks, showcase your research using Dashboards</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Using machine learning to tell apart rain, snow, hail and fog from cell tower data]]></title>
            <link>https://blog.esciencecenter.nl/using-machine-learning-to-tell-apart-rain-snow-hail-and-fog-from-cell-tower-data-6ab856c99f8b?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/6ab856c99f8b</guid>
            <category><![CDATA[weather]]></category>
            <category><![CDATA[meteorology]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[nlescssi]]></category>
            <category><![CDATA[precipitation]]></category>
            <dc:creator><![CDATA[Netherlands eScience Center]]></dc:creator>
            <pubDate>Thu, 21 Apr 2022 20:23:35 GMT</pubDate>
            <atom:updated>2022-04-22T09:52:47.978Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p>By <a href="http://www.twitter.com/kiriwhan">Kirien Whan</a> (<a href="http://www.twitter.com/knmi">KNMI</a>), <a href="http://www.twitter.com/indyvangrinsven">Indy van Grinsven</a> (<a href="http://www.twitter.com/wur">WUR</a>), <a href="http://www.twitter.com/walraven_bas">Bas Walraven</a> (<a href="http://www.twitter.com/wrmtudelft">TUD</a>), Ruben Imhoff (WUR, Deltares), Luuk van der Valk (TUD), Linda Bogerd (WUR, KNMI), <a href="https://www.esciencecenter.nl/team/dr-meiert-grootes/">Meiert Willem Grootes</a> (<a href="https://www.esciencecenter.nl/">Netherlands eScience Center</a>), Remko Uijlenhoet (TUD) and <a href="https://twitter.com/gjsteeneveld">Gert-Jan Steeneveld</a> (WUR).</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/0*wiqxGe5uqhQAk5ch" /><figcaption>Photo by <a href="https://unsplash.com/@riiyad?utm_source=medium&amp;utm_medium=referral">Kabiur Rahman Riyad</a> on <a href="https://unsplash.com?utm_source=medium&amp;utm_medium=referral">Unsplash</a></figcaption></figure><p>When you hear the words ‘microwaves’ and ‘water’ in one sentence there’s a good chance you start thinking about heating up your lunch (well, not if you’re Dutch in which case you probably just brought a cheese sandwich to work). Perhaps, if you’re in a scientific mood, you might think of a weather radar, or soil moisture remote sensing instead. But did you also know that the microwave signal between cellular communication towers can be used to estimate rainfall? Simply put, the signals between two mobile phone towers (called Commercial Microwave Links or CMLs) are reduced by the rain that falls in between these two towers. The amount of reduction in the signal, or attenuation, is recorded by mobile network operators. Based on this attenuation, we can estimate the amount of rain that has fallen.</p><p>Multiple studies have shown that the attenuation by rain of the signal along CMLs can be used successfully to estimate rainfall. One of the advantages of estimating rainfall with CMLs is that they are used globally (including in low- to middle income countries), and are often located in populated areas, allowing for researchers to measure precipitation over areas where traditional observations (such as observations retrieved from weather radars and rain gauges) are less common. CMLs are also located close to the ground, where rainfall information is needed for weather forecasting, water management and agriculture. There is also a challenge with measuring precipitation with CMLs: precipitation needs to be classified into different types, like rain (liquid), sleet, hail, or snow (all solid). Each precipitation type interferes differently with the microwave signal. Rain attenuates the signal, whereas hail, for example, is more related to scattering of the signal. During our small-scale initiative project in collaboration with the eScience Center, we used machine learning techniques to distinguish precipitation types from the CML signal.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/904/1*EoPGYTO7iQfyO_DvEUq1aA.png" /><figcaption>Figure 1: a) A map of Wageningen showing the CML path (red), positions of the disdrometers (yellow dots); b) The CML transmitting antenna mast with, from top to bottom, a near-infrared link, a Nokia CML and a research link; <strong>©</strong> A Parsivel disdrometer; <strong>(d)</strong> Tipping bucket rain gauge.</figcaption></figure><p>Our study uses data from an experimental CML set-up in Wageningen in, the Netherlands (Figure 1). The set-up contains five disdrometers, which are instruments that report the fall speed, size and type of precipitation particles (hydrometeors). These disdrometer data serve as reference to the machine learning process with measurements every 30 seconds, i.e. almost 1.5 million precipitation type observations for our study period (August 2014 till December 2015). Within each 30s interval, the CML signal is even recorded up to 600 times. Although the amount of data seems enormous and well suited for a machine learning project, the identification of precipitation types introduces an additional challenge compared to the more ‘straightforward’ wet- or dry classification. <em>It appears that it actually rains less often in the Netherlands than you might think</em> (Figure 2). With rain cases just under 10% of the time, the dataset becomes quite unbalanced with mainly dry moments. Hail and snow occur even less frequently.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/584/1*uVDrFxGzV0vcL471iYAC6Q.png" /><figcaption>Figure 2: Distribution of precipitation type in the Netherlands as an example of an unbalanced dataset.</figcaption></figure><p>To balance the dataset for the machine learning process, it should contain enough dry and wet moments, including a fair distribution of the different precipitation types, and therefore we apply <strong>data augmentation</strong>. This enables the machine learning algorithm to learn as much as possible, instead of being fed no-rain instances for most of the training period. Next, we use the NL eScience Center’s <a href="https://blog.esciencecenter.nl/mcfly-an-easy-to-use-tool-for-deep-learning-for-time-series-classification-b2ee6b9419c2"><strong>McFly</strong></a> algorithm to identify the best-suited <strong>machine learning</strong> technique for the problem at hand. <a href="https://blog.esciencecenter.nl/mcfly-an-easy-to-use-tool-for-deep-learning-for-time-series-classification-b2ee6b9419c2"><strong>McFly</strong></a> “tests” various machine learning algorithms on samples of the dataset, and picks the most suitable model for this specific problem, allowing us to go more in depth into classifying precipitation types based on CML data. However, due to the limited quality of the target dataset, so far we were not yet successful in classifying the precipitation types.</p><p>Next,<strong> </strong>we explored whether CMLs can be used to detect <strong>fog</strong>. Fog consists of droplets so small that they float in the air up to a few meters above the ground. The disdrometers do not pick up fog accurately enough to be used as target dataset. For this reason, we used an alternative target dataset to detect fog.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/684/1*uvjZmpw1nQEMWuJ_eaT72g.png" /><figcaption>Figure 3: CML attenuation signal (red) and target dataset (fog/no fog in blue) model fog prediction (fog/no fog in black) model fog threshold (dashed gray) for 31 August 2015.</figcaption></figure><p>The relatively round fog droplets can scatter a wide range of wavelengths smaller than the droplets themselves. This includes visible light, making fog… foggy. Fog also affects wavelengths within the near-infrared range. Luckily, the experimental setup of the measurement campaign also included a near-infrared link (Figure 1), which we use as reference dataset of the foggiest time steps. Comparing the fog time series and the CML signal reveals a distinct drop of the CML signal which coincides with the fog occurrence (Figure 3). <br>Once again we balance the dataset because the moments without fog far outnumber the fog events. After balancing the dataset and feeding the raw CML data into the <strong>McFly</strong> algorithm, the result is a mere 49% accuracy. For a balanced two-class dataset, this is not better than just tossing a coin…</p><p>It appears that the raw data for each individual time step is not enough to create a neural network that detects fog. However, the attenuation signal in the CML data shows a pattern associated with fog. This pattern changes over time, but <strong>McFly</strong> does not automatically take the time dimension into consideration. Hence, to account for the temporal aspects, a rolling rate of change of the CML attenuation signal over 15 minutes is included for every time step as input to <strong>McFly</strong>. On its own, this input value could be used to gain a validation accuracy of 67%, but combining it with the raw CML data brings it up to 73%. As a next step, we include additional inputs to the machine learning model to represent the time of the day and a time of the year, to allow the neural network to learn the climatology of fog. On its own, this correctly predicts fog 60% of the time steps, but when combined with the other data, the validation accuracy reaches 77%, which is a promising first step towards detecting not only rain, but also fog with CMLs.</p><p>This blog is part of our blog series: <em>The Small-Scale Initiative on Machine Learning, how did it go?, </em>where groups who were invited to participate in a project with eScience Center Research Software Engineers write about their projects and their experience.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=6ab856c99f8b" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/using-machine-learning-to-tell-apart-rain-snow-hail-and-fog-from-cell-tower-data-6ab856c99f8b">Using machine learning to tell apart rain, snow, hail and fog from cell tower data</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[A letter to my parents about my experience in a machine learning consultancy project]]></title>
            <link>https://blog.esciencecenter.nl/a-letter-to-my-parents-about-my-experience-in-a-machine-learning-consultancy-project-aa5520d63329?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/aa5520d63329</guid>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[explain]]></category>
            <category><![CDATA[experience]]></category>
            <dc:creator><![CDATA[Sven van der Burg]]></dc:creator>
            <pubDate>Thu, 14 Apr 2022 12:03:15 GMT</pubDate>
            <atom:updated>2022-04-14T12:03:13.018Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p><em>What does a Research Software Engineer do? This is not always easy to answer, but when your parents are asking, you definitely want to give a nice answer</em></p><figure><img alt="A fountain pen writing on a piece of paper." src="https://cdn-images-1.medium.com/max/1024/1*yseerb7z2bld7brhdY2y1A.jpeg" /></figure><p>Dear Mum and Dad,</p><p>I always wonder how you picture my daily work at the Netherlands eScience Center. Probably you imagine me sitting behind a computer all day. I think you know by now that I also sometimes talk to scientists, mostly from the healthcare domain. To give you a better feeling of the kind of things I do I decided to write you about my experience in the <em>Automated Video-based Assessment of Movement Disorders</em> project.</p><p>Automated Video-based Assessment of Movement Disorders… That is a mouth full of words. Let me try to explain what I did in this project.</p><p>This project is in collaboration with <a href="https://www.amsterdamumc.org/en/research/researchers/helga-haberfehlner.htm">Helga Haberfehlner</a> and her colleagues who work at the Department of Rehabilitation medicine of the Amsterdam UMC. Helga is a kind, energetic, and passionate researcher who is trying to improve the diagnosis and treatment of movement disorders in children.</p><p>One of the problems they face in the clinic is how to objectively determine how severe such a movement disorder is and how it progresses over time. This is important to know because it helps the doctors pick the right treatment, or give them an idea about whether a treatment is working. In one such method a video of the patient is recorded. The video is then observed by a doctor who is trained to carefully look at the movements of the body. The doctor gives a score between 0 and 4 for how bad a particular set of symptoms is in a particular body part. For example, the doctor could rate the symptoms in the left lower arm as a 3. If, for example, after a year of medical treatment this score goes to 1 this indicates that the treatment is working.</p><p>Now the problem is that it is quite difficult to score the symptoms of a patient. Different doctors tend to look at different things. Even the same doctor looking at the same video could give a low score on an optimistic day, and a high score on a pessimistic day. Also, it takes a lot of time for the doctors to do this correctly, time that is better spent on care for the patients instead of looking at videos. So, the doctors would be helped a lot if the scoring of these videos could be done automatically by a computer. Together with my eScience Center colleagues Florian Huber and Sonja Georgievska we set out to help Helga and her colleagues from the Amsterdam UMC.</p><p>In most of the projects at the eScience Center we build computer programs with input from the domain scientists we work together with. These are long projects, taking at least 2 years. But in this project (the so-called Small-Scale-Initiative Machine Learning project) we only had half a year in which we mostly gave advise on the activities that the researchers perform themselves. The good thing is that this is exactly what Helga needed: technological expertise. And by doing projects in such a way we could actually not just help Helga and her colleagues, but also 11 other such projects from a diverse range of scientific disciplines.</p><p>How did we help Helga and her colleagues? We mostly gave advise on a technique called ‘Machine Learning’. It is a technology that teaches a computer how to do a task by showing it examples of how to do it. In our case, the computer had to learn how to give a score between 0 and 4 indicating how bad the symptoms of a movement disorder are in the patient in the video. We managed to teach the computer how to do it reasonably well, although there is still some work needed before doctors can start using this technique. You can see our results in our <a href="https://github.com/RehabAUmc/modys-video">Github repository</a> (I might explain more about Github in another letter 😉). We are also working on writing our conclusions down in a scientific article (<strong>@dad</strong> I hope we get as much attention as <a href="https://library.wur.nl/ojs/index.php/njas/article/view/16546">you got in the glory of your soil-researching period</a>).</p><p>What I most enjoyed in this project is the fruitful collaboration that we had. I think Helga and her colleagues learned a lot from the technological expertise we could offer. Helga even started to (successfully) do more programming herself. But the other way around we learned a lot about how to apply machine learning techniques in a clinical setting. We even got a tour around Helga’s department where she showed us all the devices that they use to investigate movement disorders.</p><p>I hope this gives you a bit of a feeling for what I do at the Netherlands eScience Center.</p><p>Lots of love,</p><p>Sven</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=aa5520d63329" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/a-letter-to-my-parents-about-my-experience-in-a-machine-learning-consultancy-project-aa5520d63329">A letter to my parents about my experience in a machine learning consultancy project</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[Can machine learning help us improve stroke rehabilitation? A step towards personalized therapy]]></title>
            <link>https://blog.esciencecenter.nl/can-machine-learning-help-us-improve-stroke-rehabilitation-a-step-towards-personalized-therapy-386efb3caf05?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/386efb3caf05</guid>
            <category><![CDATA[rehabilitation]]></category>
            <category><![CDATA[nlescssi]]></category>
            <category><![CDATA[stroke]]></category>
            <category><![CDATA[machine-learning]]></category>
            <category><![CDATA[personalized-therapy]]></category>
            <dc:creator><![CDATA[Netherlands eScience Center]]></dc:creator>
            <pubDate>Thu, 07 Apr 2022 11:07:07 GMT</pubDate>
            <atom:updated>2022-04-11T12:08:04.182Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p>By <a href="https://research.vu.nl/en/persons/sina-david">Sina David</a>, <a href="https://www.internationalhu.com/research/researchers/michiel-punt">Michiel Punt</a> and <a href="https://research.vu.nl/en/persons/yuge-zhang">Yuge Yhang</a></p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*OKGyMg6ClTarvv5O553XHQ.jpeg" /><figcaption>Photo by <a href="https://unsplash.com/photos/KFIjzXYg1RM">Jeremy Bishop</a> on <a href="https://unsplash.com/">Unsplash</a></figcaption></figure><p>Experiencing a stroke was the second leading cause of death in 2019 with a worldwide number of 101.5 million strokes. Although though the number of strokes is not rising in developed countries because we got better at preventing them, the total number still increased between 1990 and 2019 by 85 per cent. Whatever we do to prevent strokes, an aging population and an increased number of people with unhealthy lifestyle (like an unhealthy diet, less daily activity and pollution) oppose prevention efforts that are going on worldwide. There’s also good news however: even though there are more and more reported strokes over the last decades, the rate at which people die from strokes is increasing less quickly.</p><p>Still, surviving a stroke can have a lot of impact on the survivor’s daily life. It can be difficult for stroke survivors to participate in the same kind of activities. Sometimes they become worse at reasoning or other cognitive functions, and they are more often affected by a mental illness like depression compared to their healthy peers. Research has shown that being able to walk allows people to actively participate in the community and to be physically independent. Because of this, improving the gait (“manner of walking”) quality of stroke survivors is one of the main goals in stroke rehabilitation.</p><p>In order to improve the quality of someone’s gait, you need markers that tell you what “good” walking is, and how to recognize “bad” walking. With modern techniques, it is possible to collect a lot of data about someone’s gait by performing a so-called gait analysis. From the data, we can calculate the gait quality markers like speed or symmetry between the left and right leg. But calculating these markers has many limitations. For example, you ignore a lot of information by calculating these markers. Furthermore, the markers are often related to each other (if you walk fast, your steps will also be more regular). So, feature-based approaches cannot reflect the complex human gait.</p><p>One way out of this could be to develop automated methods of gait analysis that can (1) fairly tell the difference between important information, and noise (2) identify patients with an ability to improve their gait quality and (3) detect the area where this improvement is realistic.</p><p>With the help of the Netherlands eScience Center Small-Scale Initiative, we hoped to find such automated methods. We want to individualize stroke rehabilitation without increasing the workload of doctors, and at the same time to make the evaluation of gait quality more objective.</p><p>While having a shallow idea of some machine learning approaches when starting the project, our meetings with the eScience Center team quickly helped us to identify methods that might be a match for our question. The discussions also helped us ask new questions and to rethink our initial research question. We experienced this as a creative process that added a lot of value to the project. After considering several methods, we decided to use variational autoencoders on our gait analysis dataset. We explored if the variational autoencoders can learn the traits of human gait and find out if the traits can be reflected in just a few features.</p><p>Along the way, the biggest issue was our data set itself. Because data collection of human movements is a job that takes time and our patient group is small, we only have a limited amount of data available. At the same time, the data set is highly variable, which makes the problem more difficult for Machine Learning.</p><p>In next steps for this project, we are exploring the results and trying to translate them into information that is valuable for our research field. At the same time, we are in the early stages of a data sharing effort, with those who may have similar data sets. That way, we will be able to increase the size of our training set.</p><p>The first result can be seen in the figure below. It shows the distribution of the different people (who were part of a study) using two latent features of a variational autoencoder. By creating the whole-body movement from the latent space, we were able to show the gait patterns which represented the people in the different areas of the two-dimensional latent space. In the future, this will help to evaluate a patient’s gait and their improvement during and after rehabilitation.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*U3Ul8q4Hh4GRlh27S1uZZQ.png" /></figure><p><a href="https://research.vu.nl/en/persons/sina-david"><strong>Dr. Sina David</strong></a> Assistant Professor in the Faculty of Behavioural and Movement Sciences, Neuromechanics and AMS-Rehabilitation &amp; Development at Vrije Universeit Amsterdam. Follow her on Twitter <a href="https://twitter.com/SinaDavid1907">@SinaDavid1907</a>.</p><p><a href="https://www.internationalhu.com/research/researchers/michiel-punt"><strong>Dr. Michiel Punt</strong></a> Senior researcher at HU University of Applied Sciences Utrecht. He is also a Postdoc researcher at VU Amsterdam. Follow him on Twitter <a href="https://twitter.com/MichielPunt">@MichielPunt</a>.</p><p><a href="https://research.vu.nl/en/persons/yuge-zhang"><strong>Yuge Yhang</strong></a> External PhD Candidate at the Faculty of Behavioural and Movement Sciences, Neuromechanics and AMS-Ageing &amp; Vitality. Follow her on Twitter <a href="https://twitter.com/yugezhang5">@yugezhang5</a>.</p><p>Learn more by visiting <a href="https://www.human-movement-sciences.nl/nm/">human-movement-sciences.nl/nm</a>.</p><h3>Acknowledgments</h3><p>The work described in this blog is supported by research software engineers (RSEs) of the Netherlands eScience Center, <a href="https://www.esciencecenter.nl/team/dr-cunliang-geng/">Dr. Cunliang Geng</a>, <a href="https://www.esciencecenter.nl/team/yang-liu/">Dr. Yang Liu</a> and Dr. <a href="https://medium.com/u/3e7d2960fe28">Sonja Georgievska</a>.</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=386efb3caf05" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/can-machine-learning-help-us-improve-stroke-rehabilitation-a-step-towards-personalized-therapy-386efb3caf05">Can machine learning help us improve stroke rehabilitation? A step towards personalized therapy</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
        <item>
            <title><![CDATA[How machine learning could help Simone to play Ludo]]></title>
            <link>https://blog.esciencecenter.nl/how-machine-learning-could-help-simone-to-play-ludo-fab95721580a?source=rss----ab3660314556---4</link>
            <guid isPermaLink="false">https://medium.com/p/fab95721580a</guid>
            <category><![CDATA[treatment]]></category>
            <category><![CDATA[nlescssi]]></category>
            <category><![CDATA[cerebral-palsy]]></category>
            <category><![CDATA[sensors]]></category>
            <category><![CDATA[machine-learning]]></category>
            <dc:creator><![CDATA[Netherlands eScience Center]]></dc:creator>
            <pubDate>Thu, 31 Mar 2022 08:48:11 GMT</pubDate>
            <atom:updated>2022-03-31T08:47:01.159Z</atom:updated>
            <cc:license>http://creativecommons.org/publicdomain/zero/1.0/</cc:license>
            <content:encoded><![CDATA[<p>By Helga Haberfehlner</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/629/1*ii34FkLcmA3bYfAhLaNzZw.jpeg" /></figure><p>Simone is a 12 year old girl. She loves to play Ludo with her mum, dad and her little brother. However, when she pushes the dice switch in the middle of the Ludo board game, she cannot properly control her movements and the tokens sometimes fall from the board. Simone also does not succeed to move the tokens, so her dad does that for her. Simone has <a href="https://cerebralpalsy.org.au/our-research/about-cerebral-palsy/what-is-cerebral-palsy/types-of-cerebral-palsy/dyskinetic-cerebral-palsy/">dyskinetic cerebral palsy</a>, due to a lack of oxygen around birth. <a href="https://www.youtube.com/watch?v=-2ODMPVjnI8">Involuntary movements</a> (called dystonia and choreo-athetosis) disturb her during daily activities such as playing Ludo.</p><p>At <a href="https://www.amc.nl/web/home.htm">Amsterdam UMC</a>, we treat children with severe dyskinetic cerebral palsy with medication that is directly delivered into the spinal canal by an indwelling pump (intrathecal baclofen). This treatment has been shown to be <a href="https://research.vumc.nl/en/publications/the-effect-of-intrathecal-baclofen-in-dyskinetic-cerebral-palsy-t">effective in reaching individual goals</a> (such as not overthrowing the tokens when playing Ludo). However, we currently have a hard time to exactly monitor the involuntary movements and the effect of treatments on them. This monitoring is very important to give the right doses of medication. During check-ups at the hospital, doctors ask parents and children, how movements evolve and observe the children in the consultation room. However, in this way, we are only able to capture a snapshot within the hospital environment of the involuntary movements. Therefore we are seeking for options to measure movements within the natural environment of the children (such as home and school), and to automatize the evaluation to make it not too time consuming for the doctors.</p><p>Activity recognition applications using sensors integrated in smartphones and smartwatches for sports and fitness are increasingly used and could possibly serve this purpose. Machine learning models integrated in the devices can distinguish between movements such as walking, jogging, climbing, running and swimming. Hence, such sensors might be a good candidate to detect and monitor the involuntary movements of Simone while playing Ludo. Furthermore, as also information extracted from videos could be useful, parents could easily film children at home with their smartphones, to obtain additional information.</p><p>We have recently started to explore the possibilities for automatic detection of involuntary movements in children with dyskinetic cerebral palsy in the home situation. <a href="https://www.zonmw.nl/nl/">ZonMw</a> financed our project <a href="https://www.zonmw.nl/nl/over-zonmw/e-health-en-ict-in-de-zorg/programmas/project-detail/imdi/home-based-measurements-of-dyskinesia-using-smartphone-coupled-inertial-sensor-technology-and-machin/">MODYS@home</a> to use smartwatch-like sensors at home for the assessment of involuntary movements. With this support, we could develop an app to measure sensor data together with synchronized videos during daily activities.</p><p>We also received help from the <a href="https://www.esciencecenter.nl/">Netherlands eScience Center</a> within the ‘Open Call for Small-Scale Initiatives in Machine Learning’. Within this project called ‘Automated video-based movement assessment using machine learning to support personalized treatment of movement disorders’, we focus on the option to use videos. We use stick figure movies extracted from real videos.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1008/1*PTuJhzxNcKIFmheeUzrJ-Q.png" /></figure><p>Real videos were selected from other projects, that had have been scored by the doctor for involuntary movements (dystonia) for the arms and legs. With these data we trained machine learning models to predict dystonia automatically.</p><p>Using machine learning within the field of clinical movement analysis was something new for us, as not only the content was new, but also the style of working with stand-up meetings and sprints, sharing codes on Github and right away publishing our dataset. We gained new knowledge and skills that brings us a step forwards to find a way to measure children with dyskinetic cerebral palsy within their home environment.</p><p>We are very excited about the results from a random forest regressor, “a traditional machine learning model” as such a model is called in the language used in machine learning world ;-) See figure for the result: the true score by the doctor is plotted against the predicted value by the model. All scores on the diagonal line are predicted completely correct. The model is not perfect yet, but it shows the potential of the method and we are looking very much forward to further improve it.</p><figure><img alt="" src="https://cdn-images-1.medium.com/max/1024/1*KmttTsBaqVJTtIK0TcEPog.png" /></figure><p>Within our projects we work within a group of clinicians, researchers and software engineers from the <a href="https://www.amsterdamumc.org/en/research/organization/about-amsterdam-umc.htm">Amsterdam UMC</a> (Annemieke Buizer, Laura Bonouvrié, Marjolein van der Krogt, Helga Haberfehlner, Shankara van der Ven, Dylan den Hartog), from the TU Delft (Jaap Harlaar), from <a href="https://moveshelf.com/">Moveshelf</a> (Ignazio Aleo, Johannes Gijsbers) and the <a href="https://www.esciencecenter.nl/">Netherlands eScience Center</a> (<a href="https://www.esciencecenter.nl/team/sven-van-der-burg/">Sven van der Burg</a>, Florian Huber and <a href="https://www.esciencecenter.nl/team/dr-sonja-georgievska/">Sonja Georgievska</a>).</p><p>If you are interested in more technical details please have a look at our <a href="https://zenodo.org/deposit/5638470">stick figure dataset</a>. All contribution to our <a href="https://github.com/RehabAUmc/modys-video">code</a> is welcome, to help Simone to play Ludo without overthrowing the tokens!</p><img src="https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=fab95721580a" width="1" height="1" alt=""><hr><p><a href="https://blog.esciencecenter.nl/how-machine-learning-could-help-simone-to-play-ludo-fab95721580a">How machine learning could help Simone to play Ludo</a> was originally published in <a href="https://blog.esciencecenter.nl">Netherlands eScience Center</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>]]></content:encoded>
        </item>
    </channel>
</rss>